{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras import utils as np_utils "
      ],
      "metadata": {
        "id": "hSXlIS80ZwTK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eGIcmruVZL2O"
      },
      "outputs": [],
      "source": [
        "#open the file\n",
        "\n",
        "with open(\"/content/drive/MyDrive/data/hold-out.txt\", \"r\") as f:\n",
        "    text = f.read().lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace '\\n' by ' '\n",
        "text = text.replace('\\n', ' ')"
      ],
      "metadata": {
        "id": "dgKLtkj7LqZM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test set\n",
        "split = int(0.9 * len(text))\n",
        "train_text = text[:split]\n",
        "test_test = text[split:]"
      ],
      "metadata": {
        "id": "aRXrqDGmL_Up"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a set of all unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "print(f'Total number of characters: {len(chars)}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzO-qtgOOdIu",
        "outputId": "9deca72e-16dd-4c91-abaf-781137ae3de7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 28.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary to map characters to integers and vice versa\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "idx_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "5IVISx_2qsNE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output encoded as integers \n",
        "\n",
        "def make_sequence(text, seq_length=40, step=3):\n",
        "    seq_in = []\n",
        "    seq_out = []\n",
        "    for i in range(0, len(text)-seq_length,1):\n",
        "        seq_in.append(text[i:(i + seq_length)])\n",
        "        seq_out.append(text[i + seq_length])\n",
        "    return seq_in, seq_out"
      ],
      "metadata": {
        "id": "iiP4wrEHq5AB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 40\n",
        "step = 3\n",
        "\n",
        "seq_in, seq_out = make_sequence(train_text, seq_length, step)\n",
        "seq_in_test, seq_out_test = make_sequence(test_test, seq_length, step=10)"
      ],
      "metadata": {
        "id": "ujj_-XMHKyYc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {len(seq_in)} train sequences and {len(seq_out)} test sequences.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91kTfoOUP4TN",
        "outputId": "48187e11-de72-4b89-ac52-865fdb08dc1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 498 train sequences and 498 test sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_in, seq_out = shuffle(seq_in, seq_out, random_state=42) #shuffle the sequences"
      ],
      "metadata": {
        "id": "6a3Lwt1oQCK5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The first sequence is `{seq_in[0]}` and the first next character is `{seq_out[0]}`.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcMtcZmDQX6y",
        "outputId": "437b8ef1-44fc-4779-cf2f-2efe08e659c0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first sequence is `stress' thrall, came there for cure and ` and the first next character is `t`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert training data to one-hot vectors\n",
        "\n",
        "n_sequences = len(seq_in)\n",
        "n_sequences_test = len(seq_in_test)\n",
        "vocab_size = len(chars)\n",
        "\n",
        "X = np.zeros((n_sequences, seq_length, vocab_size), dtype=np.float32)            \n",
        "X_test = np.zeros((n_sequences_test, seq_length, vocab_size), dtype=np.float32)         \n",
        "y = np.zeros((n_sequences, vocab_size), dtype=np.float32)\n",
        "y_test = np.zeros((n_sequences_test, vocab_size), dtype=np.float32)\n",
        "                  \n",
        "\n",
        "# Fill the training data\n",
        "for i, sequence in enumerate(seq_in):\n",
        "    y[i, char_to_int[seq_out[i]]] = 1\n",
        "    for j, char in enumerate(sequence):\n",
        "        X[i, j, char_to_int[char]] = 1\n",
        "        \n",
        "# Fill the test data\n",
        "for i, sequence in enumerate(seq_in_test):\n",
        "    y_test[i, char_to_int[seq_out_test[i]]] = 1\n",
        "    for j, char in enumerate(sequence):\n",
        "        X_test[i, j, char_to_int[char]] = 1"
      ],
      "metadata": {
        "id": "h_ccXBgTQbGx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Shape of the tensor X: {X.shape}, shape of the matrix y: {y.shape}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf8f3JvXRHEt",
        "outputId": "84064b74-a29c-4be1-cf25-dba0708caf1e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the tensor X: (498, 40, 28), shape of the matrix y: (498, 28).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute the per-character perplexity of model predictions\n",
        "\n",
        "def perplexity(y_true, y_pred):\n",
        "    likelihoods = np.sum(y_pred * y_true, axis=1)\n",
        "    return 2 ** (-np.mean(np.log2(likelihoods)))"
      ],
      "metadata": {
        "id": "E9Fq_llNRJ75"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(512, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\") #optimize the model"
      ],
      "metadata": {
        "id": "VtWy09yKRZcl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_perplexity(model, X, y):\n",
        "    predictions = model(X)\n",
        "    return perplexity(y, predictions)"
      ],
      "metadata": {
        "id": "oCLS73aBRMRz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Model perplexity on the untrained model is {model_perplexity(model, X_test, y_test)}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPEEJEy_rBOt",
        "outputId": "a3beb85b-89c5-46c7-fee5-4771b794bf8d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model perplexity on the untrained model is 27.953109766532002.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model for one epoch on a very small subset of the training set to check that it is well defined."
      ],
      "metadata": {
        "id": "2L1dYuRKRmA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_train = slice(0, None, 40)\n",
        "historic_run = model.fit(X[small_train], y[small_train], validation_split=0.1, batch_size=512, epochs=40)                  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY86COwzRiwZ",
        "outputId": "36b23e30-8847-4c9f-efba-29db16f4542b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1/1 [==============================] - 6s 6s/step - loss: 3.3364 - val_loss: 3.2946\n",
            "Epoch 2/40\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 3.2644 - val_loss: 3.2457\n",
            "Epoch 3/40\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 3.1734 - val_loss: 3.1253\n",
            "Epoch 4/40\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 3.0028 - val_loss: 2.5475\n",
            "Epoch 5/40\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 2.3236 - val_loss: 3.2048\n",
            "Epoch 6/40\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 2.7507 - val_loss: 2.2899\n",
            "Epoch 7/40\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 2.3072 - val_loss: 1.9138\n",
            "Epoch 8/40\n",
            "1/1 [==============================] - 1s 554ms/step - loss: 1.8170 - val_loss: 2.1131\n",
            "Epoch 9/40\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 2.1324 - val_loss: 2.2256\n",
            "Epoch 10/40\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 2.0509 - val_loss: 2.1495\n",
            "Epoch 11/40\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 2.0679 - val_loss: 1.9977\n",
            "Epoch 12/40\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 2.0104 - val_loss: 1.8112\n",
            "Epoch 13/40\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 1.9169 - val_loss: 1.7116\n",
            "Epoch 14/40\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 1.7564 - val_loss: 1.6767\n",
            "Epoch 15/40\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 1.9488 - val_loss: 1.6987\n",
            "Epoch 16/40\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 1.8235 - val_loss: 1.7851\n",
            "Epoch 17/40\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 1.7937 - val_loss: 1.8717\n",
            "Epoch 18/40\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 1.8815 - val_loss: 1.9276\n",
            "Epoch 19/40\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 1.7899 - val_loss: 1.9566\n",
            "Epoch 20/40\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 1.8562 - val_loss: 1.9886\n",
            "Epoch 21/40\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 1.8238 - val_loss: 2.0208\n",
            "Epoch 22/40\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 1.8327 - val_loss: 2.0410\n",
            "Epoch 23/40\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 1.7499 - val_loss: 2.0176\n",
            "Epoch 24/40\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 1.6825 - val_loss: 1.9731\n",
            "Epoch 25/40\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 1.8099 - val_loss: 1.9214\n",
            "Epoch 26/40\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 1.6957 - val_loss: 1.8894\n",
            "Epoch 27/40\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 1.6665 - val_loss: 1.8647\n",
            "Epoch 28/40\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 1.8466 - val_loss: 1.8554\n",
            "Epoch 29/40\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 1.6230 - val_loss: 1.8530\n",
            "Epoch 30/40\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 1.6217 - val_loss: 1.8529\n",
            "Epoch 31/40\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 1.7933 - val_loss: 1.8370\n",
            "Epoch 32/40\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 1.7668 - val_loss: 1.7879\n",
            "Epoch 33/40\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 1.8347 - val_loss: 1.7549\n",
            "Epoch 34/40\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 1.7068 - val_loss: 1.7291\n",
            "Epoch 35/40\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 1.6302 - val_loss: 1.7155\n",
            "Epoch 36/40\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 1.9098 - val_loss: 1.7162\n",
            "Epoch 37/40\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 1.6916 - val_loss: 1.7384\n",
            "Epoch 38/40\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 1.7037 - val_loss: 1.7698\n",
            "Epoch 39/40\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 1.6785 - val_loss: 1.8249\n",
            "Epoch 40/40\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.7932 - val_loss: 1.8779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The model perplexity on the model trained one epoch is {model_perplexity(model, X_test, y_test)}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjvkdCKYRvFP",
        "outputId": "e7ab10c2-4e34-42c0-b676-2f36dec9f7aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model perplexity on the model trained one epoch is 218.2481985324807.\n"
          ]
        }
      ]
    }
  ]
}